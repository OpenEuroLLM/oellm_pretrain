##
## CONFIGURATION FILE FOR OELLM TRAINING WITH MEGATRON-LM
##
## Sections:
## - `sbatch_args`: Slurm setup (not passed to Megatron).
## - `sweep_args`: Keys to sweep over (grid search).
## - `megatron_args`: Mapped to `pretrain_gpt.py` CLI flags.
##
## Mapping rules for `megatron_args`:
##   - `arg_name: val` -> `--arg-name val`
##   - `bool: true`   -> `--arg-name`
##   - `bool: false`  -> omitted
##   - `list: [a, b]` -> `--arg-name a b`
##
## Example:
##   megatron_args:
##     foo: 1
##     bar: [x, y]
##     flag: true
##     skip: false
##
## expands to:
##   --foo 1 --bar x y --flag
##

##################################################################################################
## SBATCH ARGS

sbatch_args:

    # Path to slurm logs and scrpts, and to checkpoints
    out_dir: "/scratch/project_462000963/users/rluukkon/git/oellm_pretrain/runs"
    # nodes: 16
    nodes: 1
    time: 00-00:10:00 # format: DD-HH:MM:SS
    # time: 02-00:00:00 # format: DD-HH:MM:SS
    account: project_462000963
    # partition: standard-g
    partition: dev-g
    
    # Job name
    job_name: "oellm_test"

##################################################################################################
## SWEEP ARGS

sweep_args: ['lr']

##################################################################################################
## MEGATRON ARGS

megatron_args:

    lr: [0.0003] #, 3e-4] # 1e-3, 5e-4
    # WANDB NAME, also used as JOB_NAME
    wandb_exp_name: oellm_a400M_2B_dryrun
    wandb_project: oellm-train

    ## Specify one of these only!
    # train_iters: null
    # train_samples: null
    train_tokens: 100_000_000_000

    ## Specify one of these only!
    # lr_warmup_iters: null
    # lr_warmup_samples: null
    lr_warmup_fraction: 0.1

    ## Specify one of these only!
    # lr_decay_iters: null
    # lr_decay_samples: null
    lr_decay_fraction: 0.2 
    
    #DATA

    # mock_data: True
    data_path: [
        1.0 /scratch/project_462000963/users/vitiugin/data_tok/hplt3_eng_cleaned_sample
        # 1.0 /scratch/project_462000963/preprocessed/gemma-3/nemotron-cc/high-actual
    ]
    data_cache_path: .data/cache

    seq_length: 2048
    max_position_embeddings: 2048
    micro_batch_size: 4
    global_batch_size: 1024
    tokenizer_type: HuggingFaceTokenizer
    tokenizer_model: "google/gemma-3-27b-pt"
    num_workers: 5
    split: '969,30,1'

    # MODEL
    num_layers: 12
    hidden_size: 1024
    ffn_hidden_size: 5760
    num_attention_heads: 16
    group_query_attention: False
    num_query_groups: 16 # no gqa when NUM_QUERY_GROUPS==NUM_ATTENTION_HEADS TODO: check
    kv_channels: 64

    moe_router_topk: 8
    moe_ffn_hidden_size: 672
    num_experts: 64
    moe_grouped_gemm: True 
    moe_aux_loss_coeff: 0.01
    # moe_z_loss_coeff: None
    
    init_method_std: 0.02
    position_embedding_type: "rope"
    rotary_base: 10000
    rotary_percent: 1.0
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-5 # rmsnorm epsilon
    qk_layernorm: False # potentially enable qk-norm
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: False
    use_flash_attn: True
    attention_softmax_in_fp32: False
    disable_bias_linear: True

    # these args substantially improve TFLOP/s/GPU (1.7B model on 54 nodes, 160 vs 140 with vs without)
    overlap_param_gather: True
    overlap_grad_reduce: True
    overlap_param_gather_with_optimizer_step: False # with interleaved pipeline parallelism

    # Fused kernels
    no_gradient_accumulation_fusion: True
    no_bias_swiglu_fusion: True
    disable_bias_linear: True
    no_bias_dropout_fusion: True

    # TRAINING, PARALLELISM
    recompute_activations: False
    use_torch_fsdp2: False # it's either this or the next 4 args
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    expert_model_parallel_size: 4
    context_parallel_size: 1
    sequence_parallel: True

    # num_layers_per_virtual_pipeline_stage: null # interleaved pipeline scheduling is only possible with pipeline parallel degree > 1.
    use_distributed_optimizer: True

    distributed_backend: nccl
    distributed_timeout_minutes: 20

    # OPTIMIZER
    optimizer: adam
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.e-8
    min_lr: 0.0
    lr_decay_style: WSD
    lr_wsd_decay_style: linear
    clip_grad: 1.0
    weight_decay: 0.05

    # PROFILE
    profile: False
    use_pytorch_profiler: False
    profile_ranks: [0]
    profile_step_start: 5
    profile_step_end: 12

    # EVALS
    eval_interval: 5000
    eval_iters: 100

    # LOGGING
    log_interval: 1
    log_progress: True
    log_throughput: True
    tensorboard_queue_size: 5

    # Set CHECKPOINT_FORMAT
    ckpt_format: "torch_dist"
    async_save: True
    load: /scratch/project_462000963/users/rluukkon/git/oellm_pretrain/checkpoints/qwen3_a400M_2B
    save: /scratch/project_462000963/users/rluukkon/git/oellm_pretrain/checkpoints/qwen3_a400M_2B 
    save_interval: 2000
    # exit_interval: 