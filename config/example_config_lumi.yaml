##
## CONFIGURATION FILE FOR OELLM TRAINING WITH MEGATRON-LM
##
## Sections:
## - `sbatch_args`: Slurm setup (not passed to Megatron).
## - `sweep_args`: Keys to sweep over (grid search).
## - `megatron_args`: Mapped to `pretrain_gpt.py` CLI flags.
##
## Mapping rules for `megatron_args`:
##   - `arg_name: val` -> `--arg-name val`
##   - `bool: true`   -> `--arg-name`
##   - `bool: false`  -> omitted
##   - `list: [a, b]` -> `--arg-name a b`
##
## Example:
##   megatron_args:
##     foo: 1
##     bar: [x, y]
##     flag: true
##     skip: false
##
## expands to:
##   --foo 1 --bar x y --flag
##

##################################################################################################
## SBATCH ARGS

sbatch_args:

    # Path to slurm logs and scrpts, and to checkpoints
    out_dir: "/scratch/project_462000963/users/rluukkon/git/oellm_pretrain/"
    nodes: 4
    time: 00:10:00 # format: DD-HH:MM:SS
    account: project_462000963
    partition: dev-g
    
    # Job name
    job_name: "oellm_test"

##################################################################################################
## SWEEP ARGS

sweep_args: ['lr']

##################################################################################################
## MEGATRON ARGS

megatron_args:

    lr: [0.0003]

    # WANDB NAME, also used as JOB_NAME
    wandb_exp_name: oellm_test
    wandb_project: oellm-train

    ## Specify one of these only!
    # train_iters: null
    # train_samples: null
    train_tokens: 300_000_000_00

    ## Specify one of these only!
    # lr_warmup_iters: null
    # lr_warmup_samples: null
    lr_warmup_fraction: 0.1

    ## Specify one of these only!
    # lr_decay_iters: null
    # lr_decay_samples: null
    lr_decay_fraction: 0.2 
    
    #DATA
    # data_path: [
        # /scratch/project_462000353/data/processed-llama31/merged/fi-culturax
    # ]
    mock_data: True

    seq_length: 2048
    max_position_embeddings: 2048
    micro_batch_size: 2
    global_batch_size: 1024
    tokenizer_type: HuggingFaceTokenizer
    tokenizer_model: "meta-llama/Llama-3.1-8B"
    num_workers: 5
    split: '99,1,0'

    # MODEL
    num_layers: 32
    hidden_size: 4096
    ffn_hidden_size: 8192
    num_attention_heads: 32
    init_method_std: 0.02
    position_embedding_type: "rope"
    rotary_base: 10000
    rotary_percent: 1.0
    group_query_attentions: False
    num_query_groups: 8 # no gqa when NUM_QUERY_GROUPS==NUM_ATTENTION_HEADS TODO: check
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-5 # rmsnorm epsilon
    qk_layernorm: False # potentially enable qk-norm
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: True
    use_flash_attn: True
    attention_softmax_in_fp32: False
    disable_bias_linear: True

    # these args substantially improve TFLOP/s/GPU (1.7B model on 54 nodes, 160 vs 140 with vs without)
    overlap_param_gather: True
    overlap_grad_reduce: True
    overlap_param_gather_with_optimizer_step: False # with interleaved pipeline parallelism

    # Fused kernels
    no_gradient_accumulation_fusion: True
    no_bias_swiglu_fusion: True
    disable_bias_linear: True
    no_bias_dropout_fusion: True

    # TRAINING, PARALLELISM
    recompute_activations: False
    use_torch_fsdp2: False # it's either this or the next 4 args
    tensor_model_parallel_size: 2
    pipeline_model_parallel_size: 1
    context_parallel_size: 1
    sequence_parallel: True

    # num_layers_per_virtual_pipeline_stage: null # interleaved pipeline scheduling is only possible with pipeline parallel degree > 1.
    use_distributed_optimizer: True

    distributed_backend: nccl
    distributed_timeout_minutes: 10

    # OPTIMIZER
    optimizer: adam
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.e-8
    min_lr: 0.0
    lr_decay_style: WSD
    lr_wsd_decay_style: linear
    clip_grad: 1.0
    weight_decay: 0.05

    # PROFILE
    profile: False
    use_pytorch_profiler: False
    profile_ranks: [0]
    profile_step_start: 5
    profile_step_end: 12

    # EVALS
    eval_interval: 72661
    eval_iters: 1

    # LOGGING
    log_interval: 1
    log_progress: True
    log_throughput: True
    tensorboard_queue_size: 5

    # Set CHECKPOINT_FORMAT
    ckpt_format: "torch" # "torch_dist" is currently bugged
    save_interval: 2000
